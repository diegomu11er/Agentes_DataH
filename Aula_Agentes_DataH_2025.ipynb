{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7-kEwHLk8z-I",
      "metadata": {
        "id": "7-kEwHLk8z-I"
      },
      "source": [
        "# Preparando o ambiente\n",
        "\n",
        "1. Instalando os pacotes\n",
        "2. Realizando os imports\n",
        "3. Definindo as configurações (Variáveis de ambiente ou Secrets)\n",
        "4. Baixando os arquivos de suporte."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7lR8erxZ_b2K",
      "metadata": {
        "id": "7lR8erxZ_b2K"
      },
      "source": [
        "## Instalando os **pacotes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J5QWFtgO_o1U",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5QWFtgO_o1U",
        "outputId": "6f934a66-dfb8-4003-d5df-ebc47d3970b3"
      },
      "outputs": [],
      "source": [
        "# Instalando todos os pacotes que vamos utilizar no curso.\n",
        "!pip install python-dotenv openai ipykernel numexpr \\\n",
        "             langchain langchain_groq langchain_google_genai \\\n",
        "             langchain_openai langchain_experimental langchain-community langchain_groq\\\n",
        "             langgraph pypdf chromadb langchain_chroma fastmcp langchain_mcp_adapters==0.0.9 \\\n",
        "             pydantic graphviz grandalf pydot requests matplotlib google-generativeai pandas tabulate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ytAqOVzB-eDq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytAqOVzB-eDq",
        "outputId": "e65d9a48-499f-4adf-b07c-2226bc2c8ec5"
      },
      "outputs": [],
      "source": [
        "# Linux\n",
        "# Instalando o graphviz, para gerarmos nossos fluxos em PNG\n",
        "!apt install graphviz graphviz-dev -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2Qcrs7upBfQz",
      "metadata": {
        "id": "2Qcrs7upBfQz"
      },
      "outputs": [],
      "source": [
        "# Windows\n",
        "# Download: https://gitlab.com/api/v4/projects/4207231/packages/generic/graphviz-releases/11.0.0/windows_10_cmake_Release_graphviz-install-11.0.0-win64.exe\n",
        "# Instalar o .exe\n",
        "# Adicionar no PATH ==> \"C:\\Program Files\\Graphviz\\bin\"\n",
        "\n",
        "# link oficial de download do instalador\n",
        "# https://graphviz.org/download/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U_09fN6n-CwW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_09fN6n-CwW",
        "outputId": "19be4d96-a291-42b8-a6fd-64f5f63181a9"
      },
      "outputs": [],
      "source": [
        "# Linux\n",
        "# Instalando o pacote do graphviz no python\n",
        "!pip install --config-settings=\"--global-option=build_ext\" --config-settings=\"--global-option=-I/usr/include/graphviz\" pygraphviz -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aFxXSSqeBv9U",
      "metadata": {
        "id": "aFxXSSqeBv9U"
      },
      "outputs": [],
      "source": [
        "# Windows\n",
        "#!pip install --config-settings=\"--global-option=build_ext\" --config-settings=\"--global-option=-IC:\\Program Files\\Graphviz\\include\" --config-settings=\"--global-option=-LC:\\Program Files\\Graphviz\\lib\" pygraphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vwFIWRmW_5m3",
      "metadata": {
        "id": "vwFIWRmW_5m3"
      },
      "source": [
        "## Importando os pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9AZyRtrk-NoD",
      "metadata": {
        "id": "9AZyRtrk-NoD"
      },
      "outputs": [],
      "source": [
        "# =======================\n",
        "# Bibliotecas da standard library (Python)\n",
        "# =======================\n",
        "\n",
        "# Sistema operacional: criação de diretórios, configuração e leitura de variáveis de ambiente\n",
        "import os\n",
        "\n",
        "# Informações e manipulação da execução do interpretador Python\n",
        "import sys\n",
        "\n",
        "# Expressões regulares\n",
        "import re\n",
        "\n",
        "# Manipulação de datas e horários\n",
        "import datetime\n",
        "\n",
        "# Execução de funções assíncronas\n",
        "import asyncio\n",
        "\n",
        "# Conexões seguras e cliente HTTP assíncrono\n",
        "import ssl\n",
        "import httpx\n",
        "\n",
        "# Manipulação de arquivos e diretórios de forma independente do sistema operacional\n",
        "from pathlib import Path\n",
        "\n",
        "# Identificadores únicos universais (UUID)\n",
        "from uuid import uuid4\n",
        "\n",
        "# Manipulação de CSV\n",
        "import pandas as pd\n",
        "\n",
        "# Tipagem estática (anotações e tipos auxiliares)\n",
        "from typing import Any, List, Union, TypedDict\n",
        "\n",
        "# Manipulação de XML\n",
        "from xml.etree import ElementTree as ET\n",
        "\n",
        "# Envio de e-mails (SMTP e MIME)\n",
        "from smtplib import SMTP, SMTP_SSL\n",
        "from email.mime.base import MIMEBase\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "from email.utils import formataddr\n",
        "try:\n",
        "    from email import encoders\n",
        "except ImportError:\n",
        "    from email import Encoders as encoders\n",
        "\n",
        "# Leitura de variáveis de ambiente a partir de arquivos `.env`\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Requisições HTTP (síncronas)\n",
        "import requests\n",
        "\n",
        "# Exibição de gráficos e imagens\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# =======================\n",
        "# OpenAI\n",
        "# =======================\n",
        "\n",
        "from openai import OpenAI\n",
        "from openai.types.chat.chat_completion import ChatCompletion\n",
        "\n",
        "# =======================\n",
        "# Gemini\n",
        "# =======================\n",
        "\n",
        "import google.generativeai as gemini\n",
        "\n",
        "# =======================\n",
        "# LangChain\n",
        "# =======================\n",
        "\n",
        "# Configuração de debug do LangChain\n",
        "from langchain.globals import set_debug\n",
        "\n",
        "# Modelos LLM (Large Language Models)\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.language_models.chat_models import BaseChatModel\n",
        "\n",
        "# Construção de prompts\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.prompts import (\n",
        "    PromptTemplate,\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "\n",
        "# Parsers de saída\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# Execução de fluxos (Runnables)\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Criação e execução de agentes\n",
        "from langchain.agents import (\n",
        "    Tool,\n",
        "    AgentExecutor,\n",
        "    create_tool_calling_agent,\n",
        "    create_react_agent\n",
        ")\n",
        "\n",
        "# Ferramentas customizadas para agentes\n",
        "from langchain.tools import tool\n",
        "from langchain_community.agent_toolkits.load_tools import load_tools\n",
        "from langchain_experimental.tools.python.tool import PythonAstREPLTool\n",
        "\n",
        "# Tipos de memória utilizados em agentes\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Componentes de RAG (Retrieval-Augmented Generation)\n",
        "from langchain_chroma import Chroma  # Armazenamento vetorial\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings  # Embeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Separador de texto\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader  # Leitura de documentos PDF\n",
        "\n",
        "# Acesso ao hub LangChain de prompts prontos (https://smith.langchain.com/hub)\n",
        "from langchain import hub\n",
        "\n",
        "# =======================\n",
        "# MCP (Model Context Protocol)\n",
        "# =======================\n",
        "\n",
        "# Servidor MCP\n",
        "from mcp.server.fastmcp import FastMCP\n",
        "\n",
        "# Cliente MCP multi-servidor\n",
        "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
        "\n",
        "# =======================\n",
        "# LangGraph\n",
        "# =======================\n",
        "\n",
        "# Criação de agentes com LangGraph\n",
        "from langgraph.prebuilt import create_react_agent as create_react_agent_graph\n",
        "\n",
        "# Sistema de checkpoint em memória\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "# Definição e execução de grafos\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# =======================\n",
        "# Outros\n",
        "# =======================\n",
        "\n",
        "# Ignora avisos durante a execução\n",
        "from IPython import get_ipython\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-VTSyM4qE-aM",
      "metadata": {
        "id": "-VTSyM4qE-aM"
      },
      "outputs": [],
      "source": [
        "# Verifica onde o notebook está rodando\n",
        "RUNNING_IN_COLAB = 'google.colab' in str(get_ipython())\n",
        "try:\n",
        "  from google.colab import userdata\n",
        "except:\n",
        "  pass\n",
        "\n",
        "OUTPUT_DOCUMENTS_DIR:str = './documentos/' if not RUNNING_IN_COLAB else '/content/documentos/'\n",
        "\n",
        "if not RUNNING_IN_COLAB and sys.platform.lower() == \"win32\" and \"Graphviz\" not in os.environ[\"PATH\"]: # Somente no Windows\n",
        "    os.environ[\"PATH\"] = os.getenv(\"PATH\", \"\") + \";C:\\\\Program Files\\\\Graphviz\\\\bin\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59L8IGZmASKL",
      "metadata": {
        "id": "59L8IGZmASKL"
      },
      "source": [
        "## Configurações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VEh8BK4T6Rcb",
      "metadata": {
        "id": "VEh8BK4T6Rcb"
      },
      "outputs": [],
      "source": [
        "# Arquivo de environment (se estiver local)\n",
        "# Exemplo de um arquivo .env\n",
        "# ==============================================\n",
        "# GROQ_API_KEY=\n",
        "# OPENAI_API_KEY=\n",
        "# ANTHROPIC_API_KEY=\n",
        "# GOOGLE_API_KEY=\n",
        "#\n",
        "# SMTP_USERNAME=\n",
        "# SMTP_PASSWORD=\n",
        "# ==============================================\n",
        "\n",
        "# Caminho de onde foi criado o .env\n",
        "ENV_PATH:str = '.env'\n",
        "\n",
        "# No colab utilizamos o <Secrets> Menu esquerda (chave)\n",
        "\n",
        "def carrega_variaveis_ambiente() -> None:\n",
        "\n",
        "    # Modo local\n",
        "    if os.path.exists(ENV_PATH) and not RUNNING_IN_COLAB:\n",
        "        load_dotenv(ENV_PATH, override=True)\n",
        "\n",
        "    # Modo Colab\n",
        "    if RUNNING_IN_COLAB:\n",
        "      os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n",
        "      os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "      os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n",
        "      os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "      os.environ['SMTP_USERNAME'] = userdata.get('SMTP_USERNAME')\n",
        "      os.environ['SMTP_PASSWORD'] = userdata.get('SMTP_PASSWORD')\n",
        "\n",
        "carrega_variaveis_ambiente()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GJMN53EuBINx",
      "metadata": {
        "id": "GJMN53EuBINx"
      },
      "source": [
        "## Baixando os arquivo para utilizarmos na Aula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CSpO_33X6RZf",
      "metadata": {
        "id": "CSpO_33X6RZf"
      },
      "outputs": [],
      "source": [
        "def download(url:str, output_dir:str=OUTPUT_DOCUMENTS_DIR) -> None:\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    filename = url.split('/')[-1]\n",
        "    filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(filepath, 'wb') as file:\n",
        "                file.write(response.content)\n",
        "            print(f\"Arquivo baixado com sucesso: {filepath}\")\n",
        "        else:\n",
        "            print(f\"Erro ao baixar o arquivo. Código de status: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OQZBzVHH6RWc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQZBzVHH6RWc",
        "outputId": "6acbe9de-42ae-48c7-d7b1-3fb1ad81f794"
      },
      "outputs": [],
      "source": [
        "download('https://middleware.datah.ai/RAG-DATA H.pdf')\n",
        "download('https://middleware.datah.ai/noticias_publicadas_ultimos_30d.csv')\n",
        "download('https://middleware.datah.ai/leitura_ultimos_5d_amostra.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D8sM5DRmGrEQ",
      "metadata": {
        "id": "D8sM5DRmGrEQ"
      },
      "source": [
        "# Agentes\n",
        "\n",
        "## Objetivo\n",
        "\n",
        "1. Entender o que é um agente\n",
        "2. Conhecer o ciclo de percepção-decisão-ação\n",
        "3. Distinguir agentes de LLMs simples\n",
        "4. Criar agentes com Langchain (LCEL / AgentExecutor)\n",
        "5. Conceitos sobre LLM (Temperatura, Top_p e Min_p)\n",
        "6. Trabalhar com Ferramentas\n",
        "7. Trabalhar com Memória\n",
        "8. Trabalhar com Contexto\n",
        "9. Prompt\n",
        "10. Trabalhar com RAG\n",
        "11. Tipos de Agentes (AgentExcutor / React)\n",
        "12. Trabalhar com MCP\n",
        "13. Trabalhar com LangGraph\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bUYSd0ASKF3L",
      "metadata": {
        "id": "bUYSd0ASKF3L"
      },
      "source": [
        "## O que é um Agente?\n",
        "\n",
        "Um agente é qualquer **entidade** que pode:\n",
        "* **Perceber** seu ambiente (ex: através de sensores)\n",
        "* **Processar** essa percepção\n",
        "* **Agir** sobre o ambiente (através de atuadores).\n",
        "\n",
        "![agente](https://middleware.datah.ai/agent_figura_11.png)\n",
        "\n",
        "## Principais Conceitos\n",
        "\n",
        "* **Percepção** (Percept): É a informação que o agente coleta do seu ambiente. Para um carro autônomo, por exemplo, a percepção são os dados das câmeras, radares e GPS.\n",
        "\n",
        "* **Ação** (Action): É o que o agente faz para interagir com o ambiente. No carro autônomo, as ações seriam acelerar, frear ou virar o volante.\n",
        "\n",
        "* **Função do Agente** (Agent Function): É o \"**cérebro**\" do agente. É a função que mapeia a sequência de percepções para uma ação. Teoricamente, **como um agente deveria agir** em resposta a uma sequência completa de percepções. Na prática, essa função é implementada por um **programa de agente**.\n",
        "\n",
        "$$f:P^* \\rightarrow A$$\n",
        "\n",
        "* **Programa do Agente** (Agent Program): É a implementação concreta da função do agente. Existem diferentes tipos de programas de agente, cada um com um nível de complexidade e \"inteligência.\n",
        "\n",
        "$$ f $$\n",
        "\n",
        "* **Atuadores** (actuators): Executam essa ação no mundo físico. Eles são a parte do agente que interage fisicamente com o ambiente.\n",
        "\n",
        "## Estrutura para criar um Agente\n",
        "\n",
        "![agente](https://middleware.datah.ai/agent_figura_01.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c9617eb",
      "metadata": {
        "id": "3c9617eb"
      },
      "source": [
        "## Criando um agente utilizando a API da OpenAI\n",
        "\n",
        "LLM + Prompt\n",
        "\n",
        "\n",
        "***IMPORTANTE: Este exemplo precisa de uma chave da OpenAI.***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fd35d92",
      "metadata": {
        "id": "5fd35d92"
      },
      "outputs": [],
      "source": [
        "# exemplo_01.py\n",
        "\n",
        "# Criando o client para trabalharmos com os agentes.\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# ou\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# Criando o agente\n",
        "def agente_manual(pergunta:str) -> str:\n",
        "    model:str = \"gpt-4o-mini\"\n",
        "    prompt:str = f\"\"\"\n",
        "Você é um assistente inteligente com acesso a duas ferramentas:\n",
        "1. Calculadora\n",
        "2. Wikipedia\n",
        "\n",
        "Dado a pergunta abaixo, diga o que pretende fazer.\n",
        "\n",
        "Pergunta: {pergunta}\n",
        "\n",
        "Responda no formato:\n",
        "Ação: [Calculadora|Wikipedia|Responder diretamente]\n",
        "Motivo: ...\n",
        "    \"\"\"\n",
        "\n",
        "    resposta:ChatCompletion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return resposta.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82d67364",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82d67364",
        "outputId": "61d103ea-687c-4ba3-ffc5-680e0378298a"
      },
      "outputs": [],
      "source": [
        "# Utilizando nosso primeiro agente\n",
        "resposta = agente_manual(\"Qual a raiz quadrada de 256?\")\n",
        "print(\"Resposta OpenAI:\",resposta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60f7641b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60f7641b",
        "outputId": "003035db-6968-45a6-eef7-ed7680ba92bb"
      },
      "outputs": [],
      "source": [
        "resposta = agente_manual(\"Qual a população de Ribeirão Preto.\")\n",
        "print(\"Resposta OpenAI:\", resposta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CLdNUt_ZcqqC",
      "metadata": {
        "id": "CLdNUt_ZcqqC"
      },
      "source": [
        "## Criando um agente utilizando a API do Gemini\n",
        "\n",
        "LLM + Prompt\n",
        "\n",
        "\n",
        "***IMPORTANTE: Este exemplo precisa de uma chave do Gemini.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZQottJDtboft",
      "metadata": {
        "id": "ZQottJDtboft"
      },
      "outputs": [],
      "source": [
        "# exemplo_01_1.py\n",
        "\n",
        "# Criando o client para trabalharmos com os agentes.\n",
        "gemini.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "\n",
        "# Criando o agente\n",
        "def agente_manual_gemini(pergunta: str) -> str:\n",
        "    model_name: str = \"gemini-1.5-flash\"\n",
        "    prompt: str = f\"\"\"\n",
        "Você é um assistente inteligente com acesso a duas ferramentas:\n",
        "1. Calculadora\n",
        "2. Wikipedia\n",
        "\n",
        "Dado a pergunta abaixo, diga o que pretende fazer.\n",
        "\n",
        "Pergunta: {pergunta}\n",
        "\n",
        "Responda no formato:\n",
        "Ação: [Calculadora|Wikipedia|Responder diretamente]\n",
        "Motivo: ...\n",
        "    \"\"\"\n",
        "\n",
        "    # Inicializa o modelo\n",
        "    model = gemini.GenerativeModel(model_name)\n",
        "    response = model.generate_content(\n",
        "        contents=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"parts\": [prompt]\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UBAAxFbXcJ13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "UBAAxFbXcJ13",
        "outputId": "7c1772de-e144-436f-bb67-9db8a1f42905"
      },
      "outputs": [],
      "source": [
        "resposta = agente_manual_gemini(\"Qual a raiz quadrada de 256?\")\n",
        "print(\"Resposta Gemini:\", resposta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NfVnPylkcOB1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "NfVnPylkcOB1",
        "outputId": "55790e05-b596-4dca-c1dd-6ed195a5ebc4"
      },
      "outputs": [],
      "source": [
        "resposta = agente_manual_gemini(\"Qual a população de Ribeirão Preto.\")\n",
        "print(\"Resposta Gemini:\", resposta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db59b93e",
      "metadata": {
        "id": "db59b93e"
      },
      "source": [
        "## LangChain\n",
        "\n",
        "O LangChain é um framework em **Python (e JS)** criado para construção de aplicações que usam LLMs (Large Language Models) como ChatGPT, Claude, Mistral, Llama, etc.\n",
        "\n",
        "\n",
        "**Porque utilizar um Framework?**\n",
        "\n",
        "* Ele facilita a criação de pipelines, chatbots, assistentes, agentes, RAGs (Retrieval-Augmented Generation), entre outros.\n",
        "\n",
        "* Facilita a portabilidade entre as LLMs. (Cada LLM tem a sua API com as sua particularidade).\n",
        "\n",
        "* Ele tem componentes prontos e bem separados (LLMs, Memory, Tools, Chains, Agents).\n",
        "\n",
        "* Facilita a construção de pipelines complexas.\n",
        "\n",
        "* Já vem com recursos de tracking, observability, serialization, etc.\n",
        "\n",
        "* Você pode usar só as partes que quiser (não é obrigatório usar tudo).\n",
        "\n",
        "Para mais detalhes acesse: https://www.langchain.com/\n",
        "\n",
        "## Iniciando com o Framework LangChain (LCEL)\n",
        "\n",
        "### O que é o LangChain Expression Language (LCEL)?\n",
        "\n",
        " **LCEL (LangChain Expression Language)** é uma ferramenta poderosa do LangChain projetada para facilitar a construção de cadeias de chamadas (chains) de forma fluida e eficiente. Pense nele como a \"cola\" que une diferentes componentes de um aplicativo de IA, como modelos de linguagem (LLMs), prompts e ferramentas, em um fluxo de trabalho coerente.\n",
        "\n",
        "A grande **vantagem do LCEL** é sua capacidade de permitir que os desenvolvedores criem pipelines complexos de maneira simples e declarativa, usando o operador | (pipe), semelhante ao que se usa em shells como Bash. Isso torna a leitura e a escrita das cadeias muito mais intuitiva.\n",
        "\n",
        "O LCEL não é apenas uma sintaxe elegante; ele traz consigo uma série de benefícios importantes:\n",
        "\n",
        "* **Streaming**: Ele suporta o streaming de tokens, ou seja, as respostas são geradas em tempo real, em vez de esperar a conclusão total da cadeia. Isso melhora a experiência do usuário, pois a resposta começa a aparecer imediatamente.\n",
        "\n",
        "* **Paralelismo**: O LCEL executa operações que não dependem umas das outras em paralelo automaticamente, o que melhora o desempenho da sua aplicação.\n",
        "\n",
        "* **Fallback**: Ele permite a definição de mecanismos de \"fallback\", onde você pode configurar um plano B caso um modelo ou ferramenta falhe, aumentando a robustez da sua aplicação.\n",
        "\n",
        "* **Composição**: A facilidade de combinar e reutilizar diferentes partes da sua cadeia, tornando o código mais modular e fácil de manter.\n",
        "\n",
        "* **Acessibilidade**: Suporte para chamadas síncronas e assíncronas, permitindo que você adapte o código ao seu ambiente.\n",
        "\n",
        "### Como o LCEL funciona?\n",
        "O funcionamento do LCEL é baseado no encadeamento de objetos que implementam a interface Runnable. Cada componente do LangChain que pode ser parte de uma cadeia – como PromptTemplate, ChatModel, OutputParser – é um Runnable.\n",
        "\n",
        "A sintaxe principal é o operador |. Quando você escreve, por exemplo, `prompt | model`, o que está acontecendo por baixo dos panos é o seguinte:\n",
        "\n",
        "* **Entrada**: A cadeia recebe uma entrada (um dicionário, uma string, etc.).\n",
        "\n",
        "* **prompt**: A entrada é processada pelo prompt. Por exemplo, uma string é formatada em um PromptValue.\n",
        "\n",
        "* **model**: O resultado do prompt é passado como entrada para o model (o LLM). O modelo, por sua vez, gera uma ChatMessage.\n",
        "\n",
        "* **Saída**: O resultado do model é a saída da cadeia.\n",
        "\n",
        "### Quando usar o LCEL?\n",
        "\n",
        "* **Construção de Cadeias Simples e Complexas**: Se você precisa conectar prompts, modelos, parsers, ferramentas ou outros componentes do LangChain em uma sequência, o LCEL é a melhor escolha. A sintaxe | é muito mais legível do que aninhar chamadas de funções.\n",
        "\n",
        "* **Aplicações que Exigem Desempenho**: Graças ao paralelismo e ao suporte a streaming, o LCEL é ideal para aplicações em produção onde o tempo de resposta é crucial, como chatbots e assistentes virtuais.\n",
        "\n",
        "* **Prototipagem Rápida**: Para testar rapidamente diferentes combinações de prompts e modelos, o LCEL permite montar e desmontar cadeias de forma ágil.\n",
        "\n",
        "* **Código Limpo e Modular**: Se você valoriza a legibilidade e a manutenção do código, o LCEL força uma estrutura mais organizada, onde cada parte da cadeia é um componente claramente definido.\n",
        "\n",
        "Vamos a um exemplo prático:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ja3W9oPDMpBm",
      "metadata": {
        "id": "ja3W9oPDMpBm"
      },
      "source": [
        "### LLM's que vamos utilizar durante todo o curso\n",
        "\n",
        "Imports\n",
        "```python\n",
        "# Modelos LLM (Large Language Models)\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.language_models.chat_models import BaseChatModel\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o28cdqOhMxm_",
      "metadata": {
        "id": "o28cdqOhMxm_"
      },
      "outputs": [],
      "source": [
        "# llms.py\n",
        "\n",
        "# Cria os modelos do Curs\n",
        "llm_padrao = ChatGroq(temperature=0, groq_api_key=os.getenv('GROQ_API_KEY'), model_name='llama-3.3-70b-versatile')        # .env/Secret = GROQ_API_KEY\n",
        "llm_openai = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")                                                               # .env/Secret = OPENAI_API_KEY\n",
        "llm_gemini = ChatGoogleGenerativeAI(temperature=0, model='gemini-1.5-flash-latest')                                       # .env/Secret = GOOGLE_API_KEY\n",
        "llm_groq_p = ChatGroq(temperature=0, groq_api_key=os.getenv('GROQ_API_KEY'), model_name='deepseek-r1-distill-llama-70b')  # .env/Secret = GROQ_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94fe2a9b",
      "metadata": {
        "id": "94fe2a9b"
      },
      "outputs": [],
      "source": [
        "# exemplo_02.py\n",
        "\n",
        "set_debug(False)\n",
        "\n",
        "# Criando o agente\n",
        "def agente_lcel(pergunta:str) -> str:\n",
        "    modelo:str = llm_padrao # Groq\n",
        "    prompt:str = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"Você é um assistente inteligente com acesso a duas ferramentas:\"),\n",
        "            (\"system\", \"1. Calculadora\"),\n",
        "            (\"system\", \"2. Wikipedia\"),\n",
        "            (\"system\", \"Dado a pergunta abaixo, diga o que pretende fazer.\"),\n",
        "            (\"human\", \"{pergunta}\"),\n",
        "            (\"system\", \"Responda no formato:\"),\n",
        "            (\"system\", \"Ação: [Calculadora|Wikipedia|Responder diretamente]\"),\n",
        "            (\"system\", \"Motivo: ...\"),\n",
        "        ])\n",
        "\n",
        "    # LCEL (LangChain Expression Language)\n",
        "    cadeia = prompt | modelo | StrOutputParser()\n",
        "    return cadeia.invoke({\"pergunta\": pergunta})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3da601e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3da601e",
        "outputId": "fa17d416-4cb9-485f-97ae-6a42af116e33"
      },
      "outputs": [],
      "source": [
        "# Utilizando nosso primeiro agente\n",
        "resposta = agente_manual(\"Qual a raiz quadrada de 256?\")\n",
        "print(resposta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec6e5b37",
      "metadata": {
        "id": "ec6e5b37"
      },
      "source": [
        "## AgentExecutor\n",
        "\n",
        "\n",
        "O **AgentExecutor** é o motor de execução de um agente. Ele é a lógica de alto nível que orquestra o processo de tomada de decisão. As principais responsabilidades do AgentExecutor são:\n",
        "\n",
        "* **Observar o Histórico de Conversas**: Ele recebe o prompt do usuário e o histórico da conversa.\n",
        "\n",
        "* **Chamar o Agent**: Ele envia essa informação para o Agent (que é um Runnable, ou seja, pode ser construído com LCEL). O Agent é a \"**mente**\" que decide a próxima ação.\n",
        "\n",
        "* **Processar a Resposta do Agente**: A resposta do Agent pode ser uma de duas coisas:\n",
        "> * **Uma AgentAction**: O agente decidiu usar uma ferramenta. O AgentExecutor então chama a ferramenta especificada com a entrada correta.\n",
        "> * **Uma AgentFinish**: O agente decidiu que a tarefa está completa e tem a resposta final para o usuário.\n",
        "\n",
        "* **Loop**: Se for uma **AgentAction**, o `AgentExecutor` executa a ferramenta, obtém o resultado e repete o processo (volta para o **passo 1**), enviando o resultado da ferramenta de volta para o agente. Ele faz isso em um loop até que o agente decida que a tarefa está finalizada (**AgentFinish**).\n",
        "\n",
        "Resumindo o `AgentExecutor` é a camada que gerencia o ciclo de vida do agente, o \"ciclo de raciocínio\".\n",
        "\n",
        "\n",
        "### **Qual é melhor utilizar o `LCEL` ou o `AgentExecutor`?**\n",
        "\n",
        "Você não precisa escolher entre LCEL e AgentExecutor. O AgentExecutor é, na verdade, uma implementação de uma cadeia construída com LCEL, porém com uma lógica de alto nível para gerenciar o ciclo de vida do agente.\n",
        "\n",
        "#### Exemplo do AgentExecutor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78908354",
      "metadata": {
        "id": "78908354"
      },
      "outputs": [],
      "source": [
        "# exemplo_02.1.py\n",
        "\n",
        "# Visualizar os detalhes da execução\n",
        "set_debug(True)\n",
        "\n",
        "def agente_langchain(pergunta:str) -> dict:\n",
        "    modelo = llm_padrao # Groq\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"Você é um assistente inteligente com acesso a duas ferramentas:\"),\n",
        "            (\"system\", \"1. Calculadora\"),\n",
        "            (\"system\", \"2. Wikipedia\"),\n",
        "            (\"system\", \"Dado a pergunta abaixo, diga o que pretende fazer.\"),\n",
        "            (\"human\", \"{pergunta}\"),\n",
        "            (\"system\", \"Responda no formato:\"),\n",
        "            (\"system\", \"Ação: [Calculadora|Wikipedia|Responder diretamente]\"),\n",
        "            (\"system\", \"Motivo: ... \"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # Onde o agente irá escrever suas anotações (Pensamento)\n",
        "        ]\n",
        "    )\n",
        "    agente = create_tool_calling_agent(modelo, tools=[], prompt=prompt)\n",
        "    executor_do_agente = AgentExecutor(agent=agente, tools=[])\n",
        "    resposta = executor_do_agente.invoke({\"pergunta\": pergunta})\n",
        "    return resposta['output']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8973c73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8973c73",
        "outputId": "3069815e-f2ca-4a20-ac8e-aebc3e9e4784"
      },
      "outputs": [],
      "source": [
        "resposta = agente_langchain(\"Qual a raiz quadrada de 256?\")\n",
        "print(resposta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccd5af2b",
      "metadata": {
        "id": "ccd5af2b"
      },
      "source": [
        "\n",
        "## Algumas vantagens do LangChain (Portabilidade)\n",
        "\n",
        "LLM + Tool + Prompt\n",
        "\n",
        "***IMPORTANTE: Este exemplo precisa de uma chave da OpenAI e Gemini.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6168e7",
      "metadata": {
        "id": "ae6168e7"
      },
      "outputs": [],
      "source": [
        "# exemplo_03.py\n",
        "\n",
        "# Visualizar os detalhes da execução\n",
        "set_debug(False)\n",
        "\n",
        "def agente_langchain(llm:BaseChatModel, pergunta:str) -> dict:\n",
        "    # https://python.langchain.com/docs/integrations/tools/\n",
        "    # https://python.langchain.com/docs/versions/migrating_chains/llm_math_chain/\n",
        "    ferramentas = load_tools([\"llm-math\"], llm=llm)\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"Você é um agente responsável por resolver problemas matemáticos.\"),\n",
        "            (\"system\", \"Utilize todas as suas ferramentas disponíveis e responda a pergunta do usuário.\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # Onde o agente irá escrever suas anotações (Pensamento)\n",
        "        ]\n",
        "    )\n",
        "    agente = create_tool_calling_agent(llm, ferramentas, prompt)\n",
        "    executor_do_agente = AgentExecutor(agent=agente, tools=ferramentas)\n",
        "    resposta = executor_do_agente.invoke({\"input\": pergunta})\n",
        "    return resposta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2524a4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2524a4d",
        "outputId": "520529f2-606f-4a24-ec39-0167d795ac8c"
      },
      "outputs": [],
      "source": [
        "# Utilizando a LLM da OpenAI para responder\n",
        "resposta = agente_langchain(llm_openai, \"Qual é a raiz quadrada de 169 vezes 2?\")\n",
        "print(f'\\n\\nResposta OpenAi: {resposta.get(\"output\", \"Não encontrei a resposta\")}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61633f1d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61633f1d",
        "outputId": "15e419da-e823-41b5-dadb-a34359dff4b0"
      },
      "outputs": [],
      "source": [
        "# Utilizando a LLM do Gemini para responder\n",
        "resposta = agente_langchain(llm_gemini, \"Qual é a raiz quadrada de 169 vezes 2?\")\n",
        "print(f'\\nResposta Gemini: {resposta.get(\"output\", \"Não encontrei a resposta\")}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mClzbfioTjX2",
      "metadata": {
        "id": "mClzbfioTjX2"
      },
      "source": [
        "## Conceitos Importantes\n",
        "\n",
        "### LLM - Temperature\n",
        "\n",
        "```python\n",
        "llm_padrao = ChatGroq(temperature=0, groq_api_key=os.getenv('GROQ_API_KEY'), model_name='llama3-70b-8192')\n",
        "llm_openai = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")                                   \n",
        "llm_gemini = ChatGoogleGenerativeAI(temperature=0, model='gemini-1.5-flash-latest')   \n",
        "```\n",
        "\n",
        "Vamos entender o que acontece quando alteramos a temperatura da LLM. Sabemos que a **LLM** prevê sempre a próxima palavra. Exemplo:\n",
        "\n",
        "```sh\n",
        "Pergunta: Como está o dia hoje?\n",
        "LLM: Hoje <>\n",
        "LLM: Hoje o <>\n",
        "LLM: Hoje o dia <>\n",
        "LLM: Hoje o dia está <>\n",
        "LLM: Hoje o dia está lindo.\n",
        "Resposta: Hoje o dia está lindo.\n",
        "```\n",
        "\n",
        "\n",
        "![temperatura](https://middleware.datah.ai/agent_figura_02.png?12)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K_eGz1VSVjsO",
      "metadata": {
        "id": "K_eGz1VSVjsO"
      },
      "source": [
        "\n",
        "\n",
        "![grafico](https://middleware.datah.ai/agent_figura_03.png?12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M1X8DH7gV8xp",
      "metadata": {
        "id": "M1X8DH7gV8xp"
      },
      "source": [
        "### Top_p (Amonstragem de núcleo)\n",
        "\n",
        "* Dada a lista de **palavras ordenadas** da **maior para menor** probabilidade.\n",
        "* Seleciona um **subconjunto** onde a soma das probabilidades é **maior ou igual ao top_p**\n",
        "* O modelo escolhe aleatoriamente uma dessas palavras.\n",
        "\n",
        "\\\n",
        "\n",
        "#### Exemplo = Top_p = 95%\n",
        "\n",
        "Hoje o dia está\n",
        "* **Lindo** 50%\n",
        "* **Bonito** 30%\n",
        "* **Claro** 15%\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "* Escuro 4%\n",
        "* Banana 1%\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lqN6A0BNYASO",
      "metadata": {
        "id": "lqN6A0BNYASO"
      },
      "source": [
        "### Min_p (Controlar a diversidade)\n",
        "\n",
        "* Recupera a **palavra com maior probabilidade**.\n",
        "* Define um limite de corte $$ \\ {p_{max} * min_p}$$\n",
        "* Seleciona um **subconjunto** onde a probabilidade é **maior que o limite de corte**.\n",
        "* O modelo escolhe aleatoriamente uma dessas palavras.\n",
        "\n",
        "#### Exemplo = Min_p = 5%\n",
        "\n",
        "Hoje o dia está\n",
        "* **Lindo** 50%    (Limite de corte 2.5%)\n",
        "* **Bonito** 30%\n",
        "* **Claro** 15%\n",
        "* **Escuro** 4%\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "* Banana 1%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8YqkLsyWYIXd",
      "metadata": {
        "id": "8YqkLsyWYIXd"
      },
      "source": [
        "## Ferramentas\n",
        "\n",
        "No contexto do LangChain, as **ferramentas** (ou tools) são funções que um modelo de linguagem (LLM) pode chamar para interagir com o mundo exterior. Pense nelas como os \"**sentidos**\" e \"**mãos**\" do seu agente de IA.\n",
        "\n",
        "Alguns exemplos de ferramentas comuns incluem:\n",
        "\n",
        "* **Busca na internet**: Uma ferramenta que usa um buscador como o Google ou o DuckDuckGo para encontrar informações atualizadas.\n",
        "* **Calculadora**: Uma ferramenta que executa operações matemáticas precisas.\n",
        "* **API de clima**: Uma ferramenta que faz uma chamada a uma API para obter a previsão do tempo para uma cidade.\n",
        "* **Leitor de arquivos**: Uma ferramenta que permite ao agente ler o conteúdo de um documento.\n",
        "* **Ferramenta de SQL**: Uma ferramenta que executa consultas em um banco de dados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce1191a6",
      "metadata": {
        "id": "ce1191a6"
      },
      "source": [
        "### Criando nossa primeira ferramenta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "120c445c",
      "metadata": {
        "id": "120c445c"
      },
      "outputs": [],
      "source": [
        "# exemplo_04.py\n",
        "\n",
        "# Visualizar os detalhes da execução\n",
        "set_debug(False)\n",
        "\n",
        "@tool\n",
        "def get_current_time(*args, **kwargs) -> str:\n",
        "    \"\"\"O objetivo dessa ferramenta é retornar a data e hora atual.\"\"\"\n",
        "    now = datetime.datetime.now()\n",
        "    return f\"A data e hora atual é {now.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "\n",
        "\n",
        "def agente_langchain(llm:BaseChatModel, usar_ferramentas:bool=True) -> dict:\n",
        "    ferramentas = [get_current_time] if usar_ferramentas else []\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"human\", \"{input}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # Onde o agente irá escrever suas anotações (Pensamento)\n",
        "        ]\n",
        "    )\n",
        "    agente = create_tool_calling_agent(llm, ferramentas, prompt)\n",
        "    executor_do_agente = AgentExecutor(agent=agente, tools=ferramentas)\n",
        "    return executor_do_agente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85c64305",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85c64305",
        "outputId": "27ccd326-69a2-4b26-a8a7-13b53583fbad"
      },
      "outputs": [],
      "source": [
        "executor_do_agente = agente_langchain(llm_padrao, usar_ferramentas=False)\n",
        "resposta = executor_do_agente.invoke({\"input\": \"Qual é a data inicial e final dessa semana?\"})\n",
        "print(f'\\n\\nResposta sem Ferramenta: {resposta.get(\"output\", \"Não encontrei a resposta\")}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0fd489",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c0fd489",
        "outputId": "94858057-1d8f-4081-b0f3-1a9587b57b2c"
      },
      "outputs": [],
      "source": [
        "set_debug(False)\n",
        "\n",
        "executor_do_agente = agente_langchain(llm_padrao, usar_ferramentas=True)\n",
        "resposta = executor_do_agente.invoke({\"input\": \"Qual é a data inicial e final dessa semana?\"})\n",
        "print(f'\\n\\nResposta com Ferramenta: {resposta.get(\"output\", \"Não encontrei a resposta\")}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f3344f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f3344f7",
        "outputId": "5bfc024b-0643-460f-902a-af94944b5084"
      },
      "outputs": [],
      "source": [
        "resposta = executor_do_agente.invoke({\"input\": \"Qual foi minha ultima pergunta?\"})\n",
        "print(f'\\n\\nResposta sem Memória: {resposta.get(\"output\", \"Não encontrei a resposta\")}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbdef138",
      "metadata": {
        "id": "bbdef138"
      },
      "source": [
        "## Memória\n",
        "\n",
        "A **memória** (ou memory) no LangChain é o componente que permite que os agentes e as cadeias de conversa retenham informações de interações anteriores. Sem a memória, cada interação seria tratada como uma nova e isolada, fazendo com que o LLM \"**esquecesse**\" o que foi dito nos turnos anteriores.\n",
        "\n",
        "A memória é crucial para construir chatbots e assistentes que podem ter conversas fluidas e contextuais. Ela injeta o histórico da conversa no prompt de cada nova chamada ao LLM, permitindo que o modelo use esse contexto para gerar respostas mais relevantes.\n",
        "\n",
        "Existem vários tipos de memória no LangChain, cada um com uma estratégia diferente para armazenar e recuperar o histórico:\n",
        "\n",
        "* **ConversationBufferMemory**: A forma mais simples de memória. Ela armazena todas as mensagens da conversa em uma variável e as injeta no prompt. É fácil de usar, mas pode se tornar ineficiente para conversas muito longas, pois o tamanho do prompt cresce.\n",
        "\n",
        "* **ConversationBufferWindowMemory**: Similar à anterior, mas armazena apenas as últimas N interações (uma \"janela\" de conversa). Isso evita que o prompt fique grande demais, mantendo apenas o contexto mais recente.\n",
        "\n",
        "* **ConversationSummaryMemory**: Em vez de armazenar a conversa inteira, ela cria um resumo contínuo das interações anteriores. Isso é ótimo para conversas longas, pois mantém o contexto sem sobrecarregar o prompt.\n",
        "\n",
        "* **ConversationSummaryBufferMemory**: Uma combinação das duas últimas, que armazena as interações recentes na íntegra e resume as interações mais antigas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QcMHOcvFezp5",
      "metadata": {
        "id": "QcMHOcvFezp5"
      },
      "source": [
        "\n",
        "## Contexto\n",
        "\n",
        "O termo \"**contexto**\" no mundo de modelos de linguagem e inteligência artificial refere-se a toda a informação relevante que um modelo precisa para entender e gerar uma resposta adequada para uma determinada solicitação. É o conjunto de dados que fornece o pano de fundo para a interação atual.\n",
        "\n",
        "Em LangChain, o **contexto é tudo aquilo que você alimenta o modelo de linguagem (LLM) junto com a pergunta** do usuário para que ele possa dar uma resposta precisa. Ele pode vir de diversas fontes e é fundamental para que o LLM não responda com base apenas em seu conhecimento pré-treinado, mas sim com base nas informações que você forneceu.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3OBsQ44IeJNL",
      "metadata": {
        "id": "3OBsQ44IeJNL"
      },
      "source": [
        "![grafico](https://middleware.datah.ai/agent_figura_04.png?12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1728857f",
      "metadata": {
        "id": "1728857f"
      },
      "outputs": [],
      "source": [
        "# exemplo_05.py\n",
        "\n",
        "# Visualizar os detalhes da execução\n",
        "set_debug(False)\n",
        "\n",
        "@tool\n",
        "def get_current_time(*args, **kwargs) -> str:\n",
        "    \"\"\"O objetivo dessa ferramenta é retornar a data e hora atual.\"\"\"\n",
        "    now = datetime.datetime.now()\n",
        "    return f\"A data e hora atual é: {now.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "\n",
        "\n",
        "def agente_langchain(llm:BaseChatModel, usar_ferramentas:bool=True) -> dict:\n",
        "    ferramentas = [get_current_time] if usar_ferramentas else []\n",
        "\n",
        "    # Retorna o histórico como uma lista de objetos de mensagem\n",
        "    memoria = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"), # O placeholder para o histórico\n",
        "            (\"human\", \"{input}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # Onde o agente irá escrever suas anotações (Pensamento)\n",
        "        ]\n",
        "    )\n",
        "    agente = create_tool_calling_agent(llm, ferramentas, prompt)\n",
        "    executor_do_agente = AgentExecutor(agent=agente, tools=ferramentas, memory=memoria)\n",
        "    return executor_do_agente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb5f88f",
      "metadata": {
        "id": "5fb5f88f"
      },
      "outputs": [],
      "source": [
        "executor_do_agente = agente_langchain(llm_padrao, usar_ferramentas=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dad5af56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dad5af56",
        "outputId": "35ca4f96-21af-4677-b117-278df330c338"
      },
      "outputs": [],
      "source": [
        "resposta = executor_do_agente.invoke({\"input\": \"Qual é a data inicial e final dessa semana?\"})\n",
        "print(f'\\n\\nResposta: {resposta.get(\"output\", \"Não encontrei a resposta\")}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30e232d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30e232d7",
        "outputId": "428293c7-3173-40cb-cbb9-3cab2b5897bb"
      },
      "outputs": [],
      "source": [
        "resposta = executor_do_agente.invoke({\"input\": \"Qual foi minha ultima pergunta?\"})\n",
        "print(f'\\n\\nResposta com Memória: {resposta.get(\"output\", \"Não encontrei a resposta\")}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c9ccd8",
      "metadata": {
        "id": "25c9ccd8"
      },
      "source": [
        "## Prompt\n",
        "\n",
        "O \"**prompt**\" é a instrução, pergunta ou texto inicial que você fornece a um modelo de linguagem (LLM) para que ele gere uma resposta. É o ponto de partida de qualquer interação com uma IA generativa.\n",
        "\n",
        "Ele é o principal meio de comunicação com a LLM, e a qualidade da sua resposta depende, em grande parte, da clareza e da precisão do prompt. Um prompt bem elaborado pode guiar o modelo a entregar exatamente o que você precisa, enquanto um prompt vago pode levar a uma resposta genérica ou irrelevante.\n",
        "\n",
        "### Tipos de Prompts\n",
        "\n",
        "Os prompts podem ser categorizados de diferentes formas, dependendo do seu formato e da informação que contêm. No contexto do LangChain e do desenvolvimento com LLMs, as duas categorias mais importantes são:\n",
        "\n",
        "* **Prompts Simples (Strings)**: Este é o tipo mais básico de prompt. É uma string de texto simples que você envia diretamente para o modelo. Não há formatação complexa ou variáveis.\n",
        "* **Prompts Estruturados (Templates)**: Este tipo de prompt é uma estrutura reutilizável, ou um template, que contém espaços reservados para variáveis. Em vez de escrever o prompt completo a cada vez, você preenche essas variáveis com dados dinâmicos. Essa abordagem é a mais utilizada em aplicações reais, pois permite criar prompts robustos e flexíveis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30d5f893",
      "metadata": {
        "id": "30d5f893"
      },
      "source": [
        "#### Prompt Simples (String)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5379892e",
      "metadata": {
        "id": "5379892e"
      },
      "outputs": [],
      "source": [
        "# exemplo_06.py\n",
        "\n",
        "# Criando o agente\n",
        "def agente(pergunta:str) -> str:\n",
        "\n",
        "    # [HumanMessage(content=pergunta)] == [(\"human\", f\"{pergunta}\")]\n",
        "    resposta = llm_padrao.invoke([HumanMessage(content=pergunta)])\n",
        "    return resposta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "442f2c5e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "442f2c5e",
        "outputId": "60e516e6-196a-4984-92ac-5b5720240e0a"
      },
      "outputs": [],
      "source": [
        "resposta = agente(\"Qual a raiz quadrada de 256?\")\n",
        "print(resposta.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c947ed67",
      "metadata": {
        "id": "c947ed67"
      },
      "source": [
        "#### Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d644d1",
      "metadata": {
        "id": "60d644d1"
      },
      "outputs": [],
      "source": [
        "# exemplo_07.py\n",
        "\n",
        "\n",
        "# Criando o agente\n",
        "def agente(pergunta:str) -> str:\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"pergunta\", \"agent_scratchpad\"],\n",
        "        template=\"\"\"Você é um assistente inteligente com acesso a duas ferramentas:\n",
        "                        1. Calculadora\n",
        "                        2. Wikipedia\n",
        "                    Dado a pergunta abaixo, diga o que pretende fazer.\n",
        "                    Pergunta: {pergunta}\n",
        "                    {agent_scratchpad}\n",
        "                    Responda no formato:\n",
        "                    Ação: [Calculadora|Wikipedia|Responder diretamente]\n",
        "                    Motivo: ...\n",
        "        \"\"\"\n",
        "    )\n",
        "    agente = create_tool_calling_agent(llm_padrao, tools=[], prompt=prompt)\n",
        "    executor_do_agente = AgentExecutor(agent=agente, tools=[])\n",
        "    resposta = executor_do_agente.invoke({\"pergunta\": pergunta})\n",
        "    return resposta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28c8954c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28c8954c",
        "outputId": "76d7710f-60c4-4a9e-812c-5e9025fcbae4"
      },
      "outputs": [],
      "source": [
        "resposta = agente(\"Qual a raiz quadrada de 256?\")\n",
        "print(resposta.get(\"output\", \"Não encontrei a resposta\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "418bc111",
      "metadata": {
        "id": "418bc111"
      },
      "source": [
        "#### ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "114d5dfc",
      "metadata": {
        "id": "114d5dfc"
      },
      "outputs": [],
      "source": [
        "# exemplo_08.py\n",
        "\n",
        "# Criando o agente\n",
        "def agente(pergunta:str) -> str:\n",
        "    prompt:str = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            SystemMessagePromptTemplate.from_template(\"Você é um assistente inteligente com acesso a duas ferramentas:\"),\n",
        "            SystemMessagePromptTemplate.from_template(\"1. Calculadora\"),\n",
        "            SystemMessagePromptTemplate.from_template(\"2. Wikipedia\"),\n",
        "            SystemMessagePromptTemplate.from_template(\"Dado a pergunta abaixo, diga o que pretende fazer.\"),\n",
        "            HumanMessagePromptTemplate.from_template(\"{pergunta}\"),\n",
        "            SystemMessagePromptTemplate.from_template(\"Responda no formato:\"),\n",
        "            SystemMessagePromptTemplate.from_template(\"Ação: [Calculadora|Wikipedia|Responder diretamente]\"),\n",
        "            SystemMessagePromptTemplate.from_template(\"Motivo: ...\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "        ])\n",
        "\n",
        "    agente = create_tool_calling_agent(llm_padrao, [], prompt)\n",
        "    executor_do_agente = AgentExecutor(agent=agente, tools=[])\n",
        "    resposta = executor_do_agente.invoke({\"pergunta\": pergunta})\n",
        "    return resposta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8d33854",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8d33854",
        "outputId": "669cc6e8-d05b-4146-e63b-d1904db096e3"
      },
      "outputs": [],
      "source": [
        "resposta = agente(\"Qual a raiz quadrada de 256?\")\n",
        "print(resposta.get(\"output\", \"Não encontrei a resposta\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c79fcb97",
      "metadata": {
        "id": "c79fcb97"
      },
      "source": [
        "## RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "**RAG**, ou **Geração Aumentada por Recuperação**, é uma técnica que combina o poder de um modelo de linguagem (LLM) com sistemas de recuperação de informações. Em termos simples, o RAG permite que o LLM acesse dados externos, como seus próprios documentos, bases de conhecimento ou a internet, antes de gerar uma resposta.\n",
        "\n",
        "O RAG **resolve três grandes problemas** dos LLMs tradicionais:\n",
        "\n",
        "1. **Conhecimento Desatualizado**: LLMs são treinados em grandes volumes de dados, mas esse conhecimento é estático e limitado à data do treinamento. O RAG permite que o modelo acesse informações em tempo real e dados que são constantemente atualizados.\n",
        "\n",
        "2. **Alucinações**: Como os LLMs às vezes inventam informações para preencher lacunas, eles podem gerar respostas incorretas ou sem fundamento. O RAG \"aterra\" a resposta em fatos concretos, usando as informações recuperadas de uma fonte externa confiável, o que reduz drasticamente a chance de alucinações.\n",
        "\n",
        "3. **Falta de Transparência**: Com o RAG, o modelo não apenas responde, mas também pode citar as fontes de onde a informação foi extraída. Isso aumenta a confiança do usuário, pois ele pode verificar a veracidade da resposta.\n",
        "\n",
        "Como o RAG funciona?\n",
        "\n",
        "![rag](https://media.geeksforgeeks.org/wp-content/uploads/20250210190608027719/How-Rag-works.webp)\n",
        "\n",
        "Mais informações https://www.geeksforgeeks.org/nlp/what-is-retrieval-augmented-generation-rag/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15bf5b61",
      "metadata": {
        "id": "15bf5b61"
      },
      "source": [
        "### Criando o Banco de dados vetorial (Chroma DB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa2938f7",
      "metadata": {
        "id": "fa2938f7"
      },
      "outputs": [],
      "source": [
        "# exemplo_09.py\n",
        "\n",
        "# Visualizar os detalhes da execução\n",
        "set_debug(False)\n",
        "\n",
        "\n",
        "def cria_banco_de_dados_vetorial(path_documentos:str) -> None:\n",
        "    try:\n",
        "        # Carrega os documentos do diretório especificado\n",
        "        documents = PyPDFDirectoryLoader(path_documentos).load()\n",
        "\n",
        "        # Usando embeddings do OpenAI\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "\n",
        "        # Cria um banco de dados vetorial usando Chroma\n",
        "        split_documents = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100).split_documents(documents)\n",
        "\n",
        "        # Cria o banco de dados vetorial\n",
        "        vectorstore = Chroma.from_documents(split_documents, embeddings, persist_directory=f'{OUTPUT_DOCUMENTS_DIR}vectorstore')\n",
        "\n",
        "        print(\"Banco de dados vetorial criado com sucesso.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar documentos: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f445e875",
      "metadata": {
        "id": "f445e875"
      },
      "outputs": [],
      "source": [
        "cria_banco_de_dados_vetorial(path_documentos=OUTPUT_DOCUMENTS_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "143d5e3c",
      "metadata": {
        "id": "143d5e3c"
      },
      "source": [
        "### Carregando o banco vetorial criado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20876fa7",
      "metadata": {
        "id": "20876fa7"
      },
      "outputs": [],
      "source": [
        "# exemplo_10.py\n",
        "\n",
        "# Visualizar os detalhes da execução\n",
        "set_debug(False)\n",
        "\n",
        "\n",
        "def carrega_banco_de_dados_vetorial(path_documentos:str) -> Chroma:\n",
        "    try:\n",
        "        # Carrega o banco de dados vetorial existente\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "        vectorstore = Chroma(persist_directory=path_documentos, embedding_function=embeddings)\n",
        "        return vectorstore\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar o banco de dados vetorial: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b5c787",
      "metadata": {
        "id": "b2b5c787"
      },
      "outputs": [],
      "source": [
        "vectorstore = carrega_banco_de_dados_vetorial(f'{OUTPUT_DOCUMENTS_DIR}vectorstore')\n",
        "docs = None\n",
        "\n",
        "if vectorstore:\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    docs = retriever.invoke(\"Data H\")\n",
        "    print(docs)\n",
        "else:\n",
        "    print(\"Não foi possível carregar o banco de dados vetorial.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edfaa1cd",
      "metadata": {
        "id": "edfaa1cd"
      },
      "source": [
        "### Criando o Agente com o RAG\n",
        "\n",
        "![rag](https://media.geeksforgeeks.org/wp-content/uploads/20250210190608027719/How-Rag-works.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "766aff4b",
      "metadata": {
        "id": "766aff4b"
      },
      "source": [
        "#### Carregando as bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63a52f6a",
      "metadata": {
        "id": "63a52f6a"
      },
      "outputs": [],
      "source": [
        "# exemplo_11.py\n",
        "\n",
        "\n",
        "# Visualizar os detalhes da execução\n",
        "set_debug(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55f115ab",
      "metadata": {
        "id": "55f115ab"
      },
      "source": [
        "#### Carregando o banco vetorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec8c118",
      "metadata": {
        "id": "0ec8c118"
      },
      "outputs": [],
      "source": [
        "def carrega_banco_de_dados_vetorial(path_documentos:str) -> Chroma:\n",
        "    try:\n",
        "        # Carrega o banco de dados vetorial existente\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "        vectorstore = Chroma(persist_directory=path_documentos, embedding_function=embeddings)\n",
        "        return vectorstore\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar o banco de dados vetorial: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "513fea0b",
      "metadata": {
        "id": "513fea0b"
      },
      "source": [
        "### Busca os dados e Cria o Contexto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f90f1945",
      "metadata": {
        "id": "f90f1945"
      },
      "outputs": [],
      "source": [
        "def busca_na_base_de_documentos(pergunta:str) -> str:\n",
        "    \"\"\"Use esta ferramenta para responder perguntas sobre a Data H, seus produtos como NIC, Consultoria, Cyber Segurança,\n",
        "       ou qualquer informação contida na base de conhecimento. A entrada deve ser a pergunta do usuário.\"\"\"\n",
        "    vectorstore = carrega_banco_de_dados_vetorial(f'{OUTPUT_DOCUMENTS_DIR}vectorstore')\n",
        "    contexto = None\n",
        "    if vectorstore:\n",
        "        retriever = vectorstore.as_retriever()\n",
        "        docs = retriever.invoke(pergunta)\n",
        "        contexto = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    return contexto\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62eb8dc9",
      "metadata": {
        "id": "62eb8dc9"
      },
      "source": [
        "#### Cria o agente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f782501",
      "metadata": {
        "id": "9f782501"
      },
      "outputs": [],
      "source": [
        "def agente_langchain(llm:BaseChatModel) -> dict:\n",
        "    ferramentas = []\n",
        "    memoria = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, input_key=\"input\") # Retorna o histórico como uma lista de objetos de mensagem\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"input\", \"context\", \"chat_history\", \"agent_scratchpad\"], # Variáveis de entrada\n",
        "        template=\"\"\"{chat_history}\n",
        "            Você é um agente de IA especializado em responder perguntas.\n",
        "            Contexto: {context}\n",
        "            Pergunta: {input}\n",
        "            {agent_scratchpad}\n",
        "        \"\"\"\n",
        "    )\n",
        "    agente = create_tool_calling_agent(llm, ferramentas, prompt)\n",
        "    executor_do_agente = AgentExecutor(agent=agente, tools=ferramentas, memory=memoria)\n",
        "    return executor_do_agente"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3656d29e",
      "metadata": {
        "id": "3656d29e"
      },
      "source": [
        "### Testando o RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea51b699",
      "metadata": {
        "id": "ea51b699"
      },
      "outputs": [],
      "source": [
        "executor_do_agente = agente_langchain(llm_padrao)\n",
        "\n",
        "pergunta = \"O que é o NIC?\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d13991b6",
      "metadata": {
        "id": "d13991b6"
      },
      "source": [
        "#### Sem contexto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61886f65",
      "metadata": {
        "id": "61886f65"
      },
      "outputs": [],
      "source": [
        "contexto = ''\n",
        "resposta = executor_do_agente.invoke({\"input\": pergunta, \"context\": contexto})\n",
        "print(f'\\n\\nResposta sem Contexto: {resposta.get(\"output\", \"Não encontrei a resposta\")}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9405366",
      "metadata": {
        "id": "f9405366"
      },
      "source": [
        "#### Com contexto RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "710ab8ac",
      "metadata": {
        "id": "710ab8ac"
      },
      "outputs": [],
      "source": [
        "contexto = busca_na_base_de_documentos(pergunta) or ''\n",
        "resposta = executor_do_agente.invoke({\"input\": pergunta, \"context\": contexto})\n",
        "print(f'\\n\\nResposta com Contexto: {resposta.get(\"output\", \"Não encontrei a resposta\")}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49d48803",
      "metadata": {
        "id": "49d48803"
      },
      "outputs": [],
      "source": [
        "pergunta = \"De qual empresa é esse produto e onde ela fica?\"\n",
        "contexto = busca_na_base_de_documentos(pergunta) or ''\n",
        "resposta = executor_do_agente.invoke({\"input\": pergunta, \"context\": contexto})\n",
        "print(f'\\n\\nResposta com Contexto e Memória: {resposta.get(\"output\", \"Não encontrei a resposta\")}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Row3klcfIabe",
      "metadata": {
        "id": "Row3klcfIabe"
      },
      "source": [
        "## Tipos de Agentes\n",
        "\n",
        "### AgentExecutor\n",
        "\n",
        "* `AgentExecutor` (orquestrador): é a classe principal no LangChain responsável por orquestrar todo o ciclo de vida de um agente. Ele é o \"**motor**\" que gerencia o fluxo de trabalho. Sua função é:\n",
        "\n",
        "\\\n",
        "\n",
        "![grafico](https://middleware.datah.ai/agent_figura_05.png?12)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y6weYRIPJGEo",
      "metadata": {
        "id": "Y6weYRIPJGEo"
      },
      "source": [
        "### ReAct\n",
        "\n",
        "* **ReAct** (O Padrão de Raciocínio) é um acrônimo para Reasoning and Acting (Raciocínio e Ação). Ele é um padrão de pensamento que um agente segue para tomar decisões. No padrão ReAct, o agente não apenas responde, ele “**pensa em voz alta**”:\n",
        "\n",
        "\\\n",
        "\n",
        "![grafico](https://middleware.datah.ai/agent_figura_06.png?12)\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "* **Pensamento**: O agente descreve o seu raciocínio. Ele analisa a pergunta e decide qual seria o próximo passo.\n",
        "* **Ação**: O agente decide qual ferramenta usar e com quais argumentos.\n",
        "* **Observação**: A saída da ferramenta. É o resultado real da ação.\n",
        "* **Resposta Final**: Quando o agente determina que a tarefa está concluída, ele para de raciocinar e fornece a resposta ao usuário.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "889EVBNqKE7q",
      "metadata": {
        "id": "889EVBNqKE7q"
      },
      "source": [
        "### Outros Tipos\n",
        "\n",
        "O **ReAct** é o padrão de raciocínio mais popular, mas o `AgentExecutor` no LangChain pode ser configurado com **outros tipos de lógica de agente**. Os mais comuns são:\n",
        "\n",
        "* **ReAct Zero-shot**: A versão mais básica do ReAct, onde o agente decide a ação com base apenas na sua capacidade de raciocínio.\n",
        "* **Conversational ReAct**: Uma extensão do ReAct que usa memória e histórico de conversa, tornando-o ideal para chatbots.\n",
        "* **OpenAI Functions / OpenAI Tools**: Este é um tipo de agente que se baseia na funcionalidade de \"**chamada de função**\" dos modelos da OpenAI. Em vez de o agente gerar um texto no formato **Thought/Action**, o próprio modelo gera uma chamada de função estruturada (**uma AgentAction**) que o `AgentExecutor` então executa. É uma abordagem mais direta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zdF8qUwGKs7j",
      "metadata": {
        "id": "zdF8qUwGKs7j"
      },
      "outputs": [],
      "source": [
        "# exemplo_12.py\n",
        "\n",
        "# Visualizar os detalhes da execução\n",
        "set_debug(True)\n",
        "\n",
        "\n",
        "def agente_langchain(llm:BaseChatModel) -> dict:\n",
        "    ferramentas = [PythonAstREPLTool()]\n",
        "\n",
        "    prompt = hub.pull(\"hwchase17/react\")\n",
        "    print('\\n','-'*40,'\\n',prompt.template, '\\n','-'*40, '\\n')\n",
        "\n",
        "    agente = create_react_agent(llm, ferramentas, prompt)\n",
        "    executor_do_agente = AgentExecutor(agent=agente, tools=ferramentas, handle_parsing_errors=True)\n",
        "    return executor_do_agente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E397cQANPQcS",
      "metadata": {
        "id": "E397cQANPQcS"
      },
      "outputs": [],
      "source": [
        "executor_do_agente = agente_langchain(llm_groq_p)  # DeepSeek R1\n",
        "\n",
        "pergunta = \"Qual é a área do triângulo com base 10 e altura 5?\"\n",
        "\n",
        "resposta = executor_do_agente.invoke({\"input\": pergunta})\n",
        "print(f'\\n\\nResposta DeepSeek R1: {resposta.get(\"output\", \"Não encontrei a resposta\")}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37fca5a7",
      "metadata": {
        "id": "37fca5a7"
      },
      "source": [
        "## MCP - Model Context Protocol\n",
        "\n",
        "O MCP (**Model Context Protocol**) é um protocolo aberto que visa padronizar a forma como aplicações de IA, como agentes, interagem com ferramentas externas e fontes de dados.\n",
        "\n",
        "Pense no MCP como um \"**adaptador universal**\" para a IA. Em vez de cada aplicação de IA ter que ser codificada para se comunicar com centenas de APIs de ferramentas diferentes, o MCP oferece uma interface comum. Isso permite que qualquer modelo de linguagem que entenda o protocolo possa usar qualquer ferramenta compatível com o MCP, independentemente de quem as criou.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jOgv1r0MQdHD",
      "metadata": {
        "id": "jOgv1r0MQdHD"
      },
      "source": [
        "### Integração sem MCP\n",
        "\n",
        "\n",
        "\\\n",
        "\n",
        "![mcp](https://middleware.datah.ai/agent_figura_07.png?12)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39A7Hfr-Q6Wq",
      "metadata": {
        "id": "39A7Hfr-Q6Wq"
      },
      "source": [
        "### Integração com MCP\n",
        "\n",
        "\\\n",
        "\n",
        "![mcp2](https://middleware.datah.ai/agent_figura_08.png?12)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19t38XU5Re0J",
      "metadata": {
        "id": "19t38XU5Re0J"
      },
      "source": [
        "### Protocolos MCP\n",
        "\n",
        "* **SSE (Server-Sent Events)**\n",
        "É um protocolo de comunicação que funciona sobre HTTP. Sua principal característica é a comunicação unidirecional, onde o servidor envia dados para o cliente em tempo real, através de uma conexão HTTP persistente.\n",
        "\n",
        "* **STDIO (Standard Input/Output)**\n",
        "É um conceito de comunicação fundamental em sistemas operacionais, não um protocolo de rede. Ele se refere aos canais de comunicação padrão de um programa: **stdin** (entrada padrão), **stdout** (saída padrão) e **stderr** (saída de erro padrão)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RTBFLlOxTOj0",
      "metadata": {
        "id": "RTBFLlOxTOj0"
      },
      "source": [
        "### Diferenças entre os protocolos\n",
        "\n",
        "| Característica | SSE                                    | STDIO                               |\n",
        "|----------------|----------------------------------------|-------------------------------------|\n",
        "| Ambiente       | Comunicação de rede (cliente-servidor) | Comunicação local (inter-processos) |\n",
        "| Fluxo          | Unidirecional (servidor -> cliente)    | Bidirecional (leitura e escrita)    |\n",
        "| Protocolo      | HTTP                                   | Canais de sistema operacional       |\n",
        "| Uso em IA      | Streaming de respostas de LLMs         | Comunicação com ferramentas locais  |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HbEvH2n_TcE4",
      "metadata": {
        "id": "HbEvH2n_TcE4"
      },
      "source": [
        "### Exemplo de utilização no Cursor AI.\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"mcpServers\": {\n",
        "      \"local-server-tools\": {\n",
        "        \"command\": \"c:/Dados/Cursos/ia/.venv/Scripts/python.exe\",\n",
        "        \"args\": [\"c:/Dados/Cursos/ia/mcp/local_server.py\"]\n",
        "      },\n",
        "      \"datah\": {\n",
        "            \"url\": \"http://127.0.0.1:5008/sse\",\n",
        "            \"transport\": \"sse\"\n",
        "      }\n",
        "  }\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QuaifCMDTxwN",
      "metadata": {
        "id": "QuaifCMDTxwN"
      },
      "source": [
        "### Vamos para o código\n",
        "\n",
        "***IMPORTANTE: Esse código deve ser rodado localmente e não no colab.***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Uin0IWw6UEEr",
      "metadata": {
        "id": "Uin0IWw6UEEr"
      },
      "source": [
        "#### Rotinas de apoio.\n",
        "\n",
        "Não deixa na ferramenta toda a responsabilidade, transforma em componentes para serem reutilizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa0ce74",
      "metadata": {
        "id": "aaa0ce74"
      },
      "outputs": [],
      "source": [
        "# mcp_helpers.py\n",
        "\n",
        "class ArxivHelper:\n",
        "\n",
        "    @property\n",
        "    def base_url(self):\n",
        "        return 'https://export.arxiv.org/api/query'\n",
        "\n",
        "    async def make_arxiv_request(self, url:str) -> str | None:\n",
        "        async with httpx.AsyncClient() as client:\n",
        "            try:\n",
        "                response = await client.get(url, headers={\"User-Agent\": 'arxiv-search-app/1.0'}, timeout=30)\n",
        "                response.raise_for_status()\n",
        "                return response.text\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                return None\n",
        "\n",
        "    def parse_arxiv_response(self, xml_data:str) -> list[dict[str, Any]]:\n",
        "        if not xml_data:\n",
        "            return []\n",
        "\n",
        "        root = ET.fromstring(xml_data)\n",
        "\n",
        "        namespaces = {\n",
        "            \"atom\": 'http://www.w3.org/2005/Atom',\n",
        "            \"arxiv\": 'http://arxiv.org/schemas/atom'\n",
        "        }\n",
        "\n",
        "        entries = []\n",
        "        for entry in root.findall('.//atom:entry', namespaces):\n",
        "            e_title = entry.find('atom:title', namespaces)\n",
        "            e_summary = entry.find('atom:summary', namespaces)\n",
        "            e_link =  entry.find('atom:id', namespaces)\n",
        "            e_published = entry.find('atom:published', namespaces)\n",
        "\n",
        "            title = e_title.text.strip() if e_title is not None else \"\"\n",
        "            summary = e_summary.text.strip() if e_summary is not None else \"\"\n",
        "\n",
        "            authors = []\n",
        "            for author in entry.findall('.//atom:author/atom:name', namespaces):\n",
        "                authors.append(author.text.strip())\n",
        "\n",
        "            link = e_link.text.strip() if e_link is not None else \"\"\n",
        "            published = e_published.text.strip() if e_published is not None else \"\"\n",
        "\n",
        "            entries.append(dict(\n",
        "                title=title,\n",
        "                summary=summary,\n",
        "                authors=authors,\n",
        "                link=link,\n",
        "                published=published\n",
        "            ))\n",
        "\n",
        "        return entries\n",
        "\n",
        "    def format_paper(self, paper:dict) -> str:\n",
        "        authors_str = \" \".join(paper.get('authors', ['Unknown author']))\n",
        "        return f\"\"\"\n",
        "            title: {paper.get('title', '')}\n",
        "            authors: {authors_str}\n",
        "            published: {paper.get('published', '')[:10]}\n",
        "            link: {paper.get('link', '')}\n",
        "            summary: {paper.get('summary', '')}\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "class MailRecipient(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.__recipients: List = []\n",
        "\n",
        "    def add(self, email: str, name: str = None):\n",
        "        if not name:\n",
        "            self.__recipients.append(email)\n",
        "        else:\n",
        "            self.__recipients.append(formataddr((name, email)))\n",
        "\n",
        "    def clear(self):\n",
        "        self.__recipients = []\n",
        "\n",
        "    def get(self) -> str:\n",
        "        return ', '.join(self.__recipients)\n",
        "\n",
        "    def has_item(self) -> bool:\n",
        "        if not self.__recipients:\n",
        "            return False\n",
        "        return len(self.__recipients) > 0\n",
        "\n",
        "\n",
        "class MailMessage(object):\n",
        "\n",
        "    def __init__(self, sender_email: str, sender_name: str = None):\n",
        "        self.__message: MIMEMultipart = MIMEMultipart()\n",
        "        self.__from: str = sender_email\n",
        "        self.__from_name: str = sender_name\n",
        "        self.__body: Union[str, None] = None\n",
        "        self.__subject: Union[str, None] = None\n",
        "        self.to: MailRecipient = MailRecipient()\n",
        "        self.cc: MailRecipient = MailRecipient()\n",
        "        self.bcc: MailRecipient = MailRecipient()\n",
        "\n",
        "    def get_message(self) -> MIMEMultipart:\n",
        "        self.__message['From'] = formataddr((self.__from_name, self.__from)) if self.__from_name else self.__from\n",
        "        self.__message['Subject'] = self.__subject\n",
        "        self.__message['To'] = self.to.get()\n",
        "        if self.cc.has_item():\n",
        "            self.__message['Cc'] = self.cc.get()\n",
        "        if self.bcc.has_item():\n",
        "            self.__message['Bcc'] = self.bcc.get()\n",
        "        self.__message.attach(self.__body)\n",
        "        self.__validate_mail_message()\n",
        "        return self.__message\n",
        "\n",
        "    def set_subject(self, subject: str):\n",
        "        self.__subject = subject\n",
        "\n",
        "    def set_text_body(self, text):\n",
        "        self.__body = MIMEText(text, \"plain\")\n",
        "\n",
        "    def set_html_body(self, html):\n",
        "        self.__body = MIMEText(html, \"html\")\n",
        "\n",
        "    def attach_file(self, filename: str, mime_type: str = \"application/octet-stream\"):\n",
        "        with open(filename, 'rb') as attachment:\n",
        "            mime_type_parts: List[str] = mime_type.split('/')\n",
        "            part: MIMEBase = MIMEBase(mime_type_parts[0], mime_type_parts[1])\n",
        "            part.set_payload(attachment.read())\n",
        "        encoders.encode_base64(part)\n",
        "        part.add_header(\"Content-Disposition\", f\"attachment; filename= {Path(filename).name}\")\n",
        "        self.__message.attach(part)\n",
        "\n",
        "    def __validate_mail_message(self):\n",
        "        if not self.__subject:\n",
        "            raise ValueError(\"The email subject is required.\")\n",
        "        if not self.__body:\n",
        "            raise ValueError(\"The email body is required.\")\n",
        "        if not self.__message['From']:\n",
        "            raise ValueError('From is required.')\n",
        "        if len(self.__message['From']) == 0:\n",
        "            raise ValueError(\"The sender (from) is required.\")\n",
        "        if not self.to.has_item() and not self.cc.has_item() and not self.bcc.has_item():\n",
        "            raise ValueError(\"Add at least a email to send this message.\")\n",
        "\n",
        "\n",
        "class SMTPServer(object):\n",
        "\n",
        "    def __init__(self, host: str, port: int = 587,\n",
        "                 username: str = None, password: str = None,\n",
        "                 has_ssl: bool = False, has_tls: bool = True,\n",
        "                 has_authentication: bool = True):\n",
        "\n",
        "        self.__host: str = host\n",
        "        self.__port: int = port\n",
        "        self.__username: str = username\n",
        "        self.__password: str = password\n",
        "        self.__tls: bool = has_tls\n",
        "        self.__ssl: bool = has_ssl\n",
        "        self.__authentication: bool = has_authentication\n",
        "        self.__context = ssl.create_default_context()\n",
        "\n",
        "    def connect(self) -> Union[SMTP, SMTP_SSL]:\n",
        "        try:\n",
        "            if self.__tls:\n",
        "                self.__server = SMTP(host=self.__host, port=self.__port)\n",
        "                self.__server.ehlo()\n",
        "                self.__server.starttls(context=self.__context)\n",
        "                self.__server.ehlo()\n",
        "            elif self.__ssl:\n",
        "                self.__server = SMTP_SSL(host=self.__host, port=self.__port, context=self.__context)\n",
        "            else:\n",
        "                self.__server = SMTP(host=self.__host, port=self.__port)\n",
        "\n",
        "            if self.__authentication:\n",
        "                self.__server.login(user=self.__username, password=self.__password)\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.__server:\n",
        "                self.disconnect()\n",
        "            raise e\n",
        "        return self.__server\n",
        "\n",
        "    def disconnect(self):\n",
        "        self.__check_connection()\n",
        "        self.__server.quit()\n",
        "\n",
        "    def send(self, mail_message: MailMessage) -> bool:\n",
        "        self.__check_connection()\n",
        "        self.__server.send_message(msg=mail_message.get_message())\n",
        "        return True\n",
        "\n",
        "    def get_sender(self) -> str:\n",
        "        return self.__username\n",
        "\n",
        "    def __check_connection(self):\n",
        "        if not self.__server:\n",
        "            raise ConnectionError(\"The server is not connected. Connect first.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2NRhYpaAVmXc",
      "metadata": {
        "id": "2NRhYpaAVmXc"
      },
      "source": [
        "#### Criando o Servidor MCP\n",
        "\n",
        "***IMPORTANTE: Não dá para rodar no colab, apenas local.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mEu4_BkSVl0g",
      "metadata": {
        "id": "mEu4_BkSVl0g"
      },
      "outputs": [],
      "source": [
        "# mcp_server.py\n",
        "\n",
        "async def search_arxiv_tool(query:str, max_results:int = 5) -> str:\n",
        "    \"\"\"\n",
        "    Esta ferramenta busca por artigos científicos no site arxiv.org\n",
        "\n",
        "    Args:\n",
        "        query (str): O assunto que deseja buscar no site.\n",
        "        max_results (int, optional): Quantidade máxima de artigos retornados pelo site. O padrão é 5.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        arxiv_mcp_tool = ArxivHelper()\n",
        "        formatted_query = query.replace(\" \", '+')\n",
        "\n",
        "        url = f\"{arxiv_mcp_tool.base_url}?search_query=all:{formatted_query}&start=0&max_results={max_results}\"\n",
        "        xml_data = await arxiv_mcp_tool.make_arxiv_request(url)\n",
        "        if not xml_data:\n",
        "            raise ValueError(\"Não foi capaz de recuperar os dados do arxiv.\")\n",
        "\n",
        "        papers = arxiv_mcp_tool.parse_arxiv_response(xml_data)\n",
        "        if not papers:\n",
        "            return FileNotFoundError(\"Artigos não encontrados.\")\n",
        "\n",
        "        paper_texts = [arxiv_mcp_tool.format_paper(paper) for paper in papers]\n",
        "        return \"\\n---\\n\".join(paper_texts)\n",
        "    except Exception as e:\n",
        "        return f\"ERRO: {str(e)}\"\n",
        "\n",
        "\n",
        "async def send_mail(subject:str, email_to:str, email_content:str, email_attach_file:str=None) -> str:\n",
        "    \"\"\"\n",
        "    Esta ferramenta envia um e-mail para uma pessoa.\n",
        "\n",
        "    Args:\n",
        "        subject (str): É o assunto do email, e deve ser um título curto.\n",
        "        email_to (str): É o e-mail para quem será enviado o email. Este argumento pode receber mais de um email, em uma string separados por \",\" or \";\". Ex: usuario1@domain.com, usuario2@domain.com\n",
        "        email_content (str): É o conteúdo do email\n",
        "        email_attach_file (str, optional): É o caminho absoluto de um arquivo para anexar ao email. Se não existir anexo, o valor deve ser None. O valor padrão é None.\n",
        "    \"\"\"\n",
        "    username=os.getenv('SMTP_USERNAME')\n",
        "    password=os.getenv('SMTP_PASSWORD')\n",
        "    smtp = SMTPServer(host='smtp.gmail.com', port=587, username=username, password=password, has_ssl=True, has_tls=True, has_authentication=True)\n",
        "\n",
        "    sender_email = 'marcelopiovan@gmail.com'\n",
        "    sender_name = 'Marcelo Piovan'\n",
        "\n",
        "    message = MailMessage(sender_email=sender_email, sender_name=sender_name)\n",
        "    message.set_subject(subject=subject)\n",
        "    for email in re.split(r'[,;]', email_to):\n",
        "        email = email.strip()\n",
        "        if email:\n",
        "            message.to.add(email=email)\n",
        "\n",
        "    message.set_html_body(email_content)\n",
        "\n",
        "    if email_attach_file is not None and email_attach_file != '' and len(email_attach_file) > 0:\n",
        "        if not os.path.isfile(email_attach_file):\n",
        "            raise FileNotFoundError(f\"Arquivo não encontrado: {email_attach_file}\")\n",
        "        message.attach_file(filename=email_attach_file)\n",
        "\n",
        "    try:\n",
        "        smtp.connect()\n",
        "        smtp.send(message)\n",
        "        smtp.disconnect()\n",
        "        return 'Email enviado com sucesso!'\n",
        "    except Exception as e:\n",
        "        return f\"ERRO: {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OI-GIjOZWtVJ",
      "metadata": {
        "id": "OI-GIjOZWtVJ"
      },
      "source": [
        "#### Rodando o servidor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PRggCvjhWaOh",
      "metadata": {
        "id": "PRggCvjhWaOh"
      },
      "outputs": [],
      "source": [
        "if RUNNING_IN_COLAB:\n",
        "    raise NotImplementedError('Esse servidor deve rodar localmente.')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🚀 Iniciando o servidor ... \")\n",
        "    mcp = FastMCP(name=\"DataH_MCP\", port=5008)\n",
        "    print(f'URL para verificação \"http://localhost:5008/sse\"')\n",
        "    mcp.add_tool(search_arxiv_tool)\n",
        "    mcp.add_tool(send_mail)\n",
        "    mcp.run(transport='sse')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cp-LtF7QWz2R",
      "metadata": {
        "id": "cp-LtF7QWz2R"
      },
      "source": [
        "### Usando um Servidor MCP - Cliente\n",
        "\n",
        "***IMPORTANTE: Não dá para rodar no colab, apenas local.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GLfFLYh1W9dH",
      "metadata": {
        "id": "GLfFLYh1W9dH"
      },
      "outputs": [],
      "source": [
        "# mcp_client.py\n",
        "\n",
        "set_debug(False)\n",
        "\n",
        "server_params = {\n",
        "    # \"local-server-tools\": {\n",
        "    #     \"command\": \"C:\\\\Dados\\\\Projetos\\\\aulas\\\\agent\\\\.venv\\\\Scripts\\\\python.exe\",\n",
        "    #     \"args\": [\"C:/Dados/Projetos/aulas/agent/src/mcp_server.py\"],\n",
        "    #     \"transport\": \"stdio\",\n",
        "    # },\n",
        "    \"server-tools\": {\n",
        "        \"url\": \"http://127.0.0.1:5008/sse\",\n",
        "        \"transport\": \"sse\",\n",
        "    }\n",
        "}\n",
        "\n",
        "async def run_agent():\n",
        "    model:str = \"gpt-4o-mini\"\n",
        "    checkpointer = InMemorySaver()\n",
        "\n",
        "    async with MultiServerMCPClient(server_params) as client:\n",
        "\n",
        "        all_tools = client.get_tools()\n",
        "        if not all_tools:\n",
        "            print(\"\\033[31mNenhuma ferramenta disponível no servidor MCP.\\033[0m\")\n",
        "\n",
        "        for server, tools in client.server_name_to_tools.items():\n",
        "            print(f'\\033[31m\\n==== MCP Server UP! - {server} ====\\033[0m')\n",
        "            for tool in tools:\n",
        "                print(f'\\033[35m* {tool.name} *\\033[0m\\n{tool.description}\\n')\n",
        "\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "            Sua tarefa é solucionar as perguntas do usuário, usando as ferramentas disponíveis e seu próprio conhecimento.\n",
        "            Responda sempre em português.\n",
        "        \"\"\"\n",
        "        agent = create_react_agent_graph(model, all_tools, checkpointer=checkpointer, prompt=prompt)\n",
        "\n",
        "        session_id = str(uuid4())\n",
        "        config = {\"configurable\": {\"thread_id\": session_id}}\n",
        "\n",
        "        while True:\n",
        "            user_input = input(\"\\033[33mFaça a sua pergunta: \\033[0m\")\n",
        "\n",
        "            if user_input == \"sair\":\n",
        "                break\n",
        "\n",
        "            if user_input == \"limpar\":\n",
        "                print(\"\\033c\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\033[34mUsuário: {user_input}\\033[0m\")\n",
        "\n",
        "            agent_response = await agent.ainvoke({\"messages\": user_input}, config=config)\n",
        "            print(f\"\\033[32mAgente: {agent_response['messages'][-1].content}\\033[0m\")\n",
        "\n",
        "            checkpoint = await checkpointer.aget(config)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-HQkHyWGYMQu",
      "metadata": {
        "id": "-HQkHyWGYMQu"
      },
      "source": [
        "#### Rodando o Cliente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rIDu5p2ZYLyq",
      "metadata": {
        "id": "rIDu5p2ZYLyq"
      },
      "outputs": [],
      "source": [
        "if RUNNING_IN_COLAB:\n",
        "    raise NotImplementedError('Esse cliente deve rodar localmente.')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    result = asyncio.run(run_agent())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zuT8rQjla-s_",
      "metadata": {
        "id": "zuT8rQjla-s_"
      },
      "source": [
        "## LangGraph\n",
        "\n",
        "O **LangGraph** é uma biblioteca construída sobre o LangChain que serve para criar agentes e **fluxos de trabalho multi-etapa** de forma mais robusta e controlada. Em vez de modelar um agente como uma simples cadeia linear, o LangGraph o representa como um grafo de estados, permitindo criar lógicas complexas com nós e arestas.\n",
        "\n",
        "\\\n",
        "\n",
        "![graph](https://middleware.datah.ai/graph.png?12)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R-chXUfdbb1b",
      "metadata": {
        "id": "R-chXUfdbb1b"
      },
      "source": [
        "\n",
        "\n",
        "Pense no `AgentExecutor` como um motor de carro que só sabe seguir um caminho em **linha reta** (o loop de raciocínio ReAct).\n",
        "\n",
        "O `LangGraph` é como um **sistema de navegação completo** que permite ao motorista escolher caminhos diferentes, fazer retornos, parar em pontos de interesse e até mesmo mudar de plano no meio da jornada.\n",
        "\n",
        "O `LangGraph` é ideal para construir agentes que precisam de lógica de controle complexa. Ele é a ferramenta para situações onde um simples AgentExecutor se torna limitado, como:\n",
        "\n",
        "* **Lógica Condicional**: Criar agentes que podem tomar decisões de \"se/então\" em cada passo.\n",
        "\n",
        ">> *Ex: Se a busca na ferramenta A falhar, tente a ferramenta B.*\n",
        "\n",
        "* **Múltiplos Fluxos de Trabalho**: Modelar um agente que pode executar diferentes tarefas com base no input inicial.\n",
        "\n",
        ">> *Ex: Se a pergunta for sobre finanças, siga um fluxo. Se for sobre atendimento ao cliente, siga outro.*\n",
        "\n",
        "* **Ciclos de Conversa Complexos**: Gerenciar conversas que precisam de aprovação do usuário, feedback ou validação antes de continuar para o próximo passo.\n",
        "\n",
        "* **Agentes de Longo Prazo**: Modelar sistemas que precisam manter um estado persistente e complexo ao longo de várias interações, como agentes que acompanham o progresso de um projeto.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CC1x2eWacjkF",
      "metadata": {
        "id": "CC1x2eWacjkF"
      },
      "source": [
        "O `LangGraph` usa o conceito de **nós** (as funções ou ações a serem executadas) e **arestas** (as transições entre os nós), com a habilidade de definir **condicional_edges** para ramificações dinâmicas no fluxo.\n",
        "\n",
        "\\\n",
        "\n",
        "![grafico](https://middleware.datah.ai/agent_figura_09.png?12)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "boax5_hznG90",
      "metadata": {
        "id": "boax5_hznG90"
      },
      "source": [
        "| Vantagens                                                                                                                                                                             | Desvantagens                                                                                                                                                     |\n",
        "|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| Controle Total do Fluxo: Você define cada etapa e transição do agente, eliminando o \"efeito caixa preta\" do AgentExecutor.                                                            | Complexidade Inicial: O aprendizado e a configuração inicial são mais complexos.                                                                                 |\n",
        "| Robustez: É mais fácil de criar mecanismos de recuperação e tratamento de erros, direcionando o fluxo para um nó de tratamento de erros em caso de falha.                             | Não é para Todos os Casos: Para agentes ReAct simples ou cadeias lineares, o AgentExecutor com LCEL é mais do que suficiente e muito mais rápido de implementar. |\n",
        "| Estado Gerenciado: Ele gerencia o estado completo do agente (incluindo o histórico de conversa e saídas de ferramentas) em um único objeto de State, tornando a depuração mais fácil. | Curva de Aprendizagem: Requer uma compreensão de conceitos de grafos e máquinas de estado, o que pode ser um obstáculo para iniciantes.                          |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_k9usmU9YWK9",
      "metadata": {
        "id": "_k9usmU9YWK9"
      },
      "outputs": [],
      "source": [
        "# exemplo_13.py\n",
        "\n",
        "# Visualizar os detalhes da execução\n",
        "set_debug(False)\n",
        "\n",
        "\n",
        "# Formatação das respostas\n",
        "def formatar_classificacao_para_estado(classificacao: str):\n",
        "    palavra_chave = classificacao.lower().strip().split()[0]\n",
        "    return {\"classificacao\": palavra_chave}\n",
        "\n",
        "def formatar_resposta_caes(resposta: str):\n",
        "    return {\"resposta\": resposta}\n",
        "\n",
        "def formatar_resposta_gatos(resposta: str):\n",
        "    return {\"resposta\": resposta}\n",
        "\n",
        "\n",
        "\n",
        "# Cadeias para cada especialidade\n",
        "def cadeia_cachorro(llm):\n",
        "    prompt_caes = ChatPromptTemplate.from_template(\n",
        "        \"Você é um especialista em cães. Responda a pergunta a seguir de forma concisa: {pergunta}\"\n",
        "    )\n",
        "    return prompt_caes | llm | StrOutputParser() | RunnableLambda(formatar_resposta_caes)\n",
        "\n",
        "\n",
        "def cadeia_gato(llm):\n",
        "    prompt_gatos = ChatPromptTemplate.from_template(\n",
        "        \"Você é um especialista em gatos. Responda a pergunta a seguir de forma concisa: {pergunta}\"\n",
        "    )\n",
        "    return prompt_gatos | llm | StrOutputParser() | RunnableLambda(formatar_resposta_gatos)\n",
        "\n",
        "\n",
        "def cadeia_classificador(llm):\n",
        "    classificador_prompt = PromptTemplate.from_template(\n",
        "        \"\"\"Classifique a seguinte pergunta como 'caes' ou 'gatos'.\n",
        "            Responda apenas com a palavra 'caes' ou 'gatos'.\n",
        "        Pergunta: {pergunta}\n",
        "\n",
        "        Tópico:\"\"\"\n",
        "    )\n",
        "    return classificador_prompt | llm | StrOutputParser() | RunnableLambda(formatar_classificacao_para_estado)\n",
        "\n",
        "\n",
        "# Define o estado do nosso grafo\n",
        "class GrafoState(TypedDict):\n",
        "    pergunta: str\n",
        "    classificacao: str\n",
        "    resposta: str\n",
        "\n",
        "\n",
        "# Condicional para rotear a pergunta\n",
        "def rotear_pergunta(state):\n",
        "    if \"caes\" in state[\"classificacao\"].lower():\n",
        "        return \"cadeia_caes\"\n",
        "    elif \"gatos\" in state[\"classificacao\"].lower():\n",
        "        return \"cadeia_gatos\"\n",
        "    else:\n",
        "        # Padrão para cães se não conseguir classificar\n",
        "        return \"cadeia_caes\"\n",
        "\n",
        "\n",
        "def fluxo(llm):\n",
        "    workflow = StateGraph(GrafoState)\n",
        "\n",
        "    # Adiciona os nós (etapas)\n",
        "    workflow.add_node(\"classificador\", cadeia_classificador(llm))\n",
        "    workflow.add_node(\"cadeia_caes\", cadeia_cachorro(llm))\n",
        "    workflow.add_node(\"cadeia_gatos\", cadeia_gato(llm))\n",
        "\n",
        "    # O início do grafo\n",
        "    workflow.set_entry_point(\"classificador\")\n",
        "\n",
        "    workflow.add_conditional_edges(\n",
        "        \"classificador\",\n",
        "        rotear_pergunta,\n",
        "        {\n",
        "            \"cadeia_caes\": \"cadeia_caes\",\n",
        "            \"cadeia_gatos\": \"cadeia_gatos\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # E os pontos de saída\n",
        "    workflow.add_edge(\"cadeia_caes\", END)\n",
        "    workflow.add_edge(\"cadeia_gatos\", END)\n",
        "\n",
        "    # Compila o grafo para uso\n",
        "    return workflow.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I7Y-mLryoLDY",
      "metadata": {
        "id": "I7Y-mLryoLDY"
      },
      "outputs": [],
      "source": [
        "workflow = fluxo(llm_padrao)\n",
        "\n",
        "# Mostra o fluxo\n",
        "workflow.get_graph().draw_png(\"graph.png\")\n",
        "workflow.get_graph().print_ascii()\n",
        "\n",
        "img = mpimg.imread('graph.png')\n",
        "imgplot = plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.title('Graph')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fLj9MJqzomDx",
      "metadata": {
        "id": "fLj9MJqzomDx"
      },
      "outputs": [],
      "source": [
        "# Executando o grafo com uma pergunta sobre cães\n",
        "pergunta_caes = \"Por que os cachorros gostam tanto de brincar de buscar?\"\n",
        "resultado_caes = workflow.invoke({\"pergunta\": pergunta_caes})\n",
        "print(f\"Pergunta: {pergunta_caes}\")\n",
        "print(f\"Resposta: {resultado_caes['resposta']}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NP2P3428oozU",
      "metadata": {
        "id": "NP2P3428oozU"
      },
      "outputs": [],
      "source": [
        "# Executando o grafo com uma pergunta sobre gatos\n",
        "pergunta_gatos = \"Qual é o som mais comum que os gatos fazem?\"\n",
        "resultado_gatos = workflow.invoke({\"pergunta\": pergunta_gatos})\n",
        "print(f\"Pergunta: {pergunta_gatos}\")\n",
        "print(f\"Resposta: {resultado_gatos['resposta']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KZQfg5DC_al0",
      "metadata": {
        "id": "KZQfg5DC_al0"
      },
      "source": [
        "## Agente analisador de CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7Gwg696wpmSX",
      "metadata": {
        "id": "7Gwg696wpmSX"
      },
      "outputs": [],
      "source": [
        "set_debug(False)\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_current_time(*args, **kwargs) -> str:\n",
        "    \"\"\"O objetivo dessa ferramenta é retornar a data e hora atual.\"\"\"\n",
        "    now = datetime.datetime.now()\n",
        "    return f\"A data e hora atual é: {now.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "\n",
        "\n",
        "def dataframe_python_code(df) -> str:\n",
        "    return Tool(\n",
        "                name=\"Códigos Python\",\n",
        "                func=PythonAstREPLTool(locals={\"df\": df}),\n",
        "                description=\"\"\"Utilize esta ferramenta sempre que o usuário solicitar cálculos, consultas, análises ou transformações\n",
        "                específicas usando Python diretamente sobre o DataFrame `df`.\n",
        "                Exemplos de uso incluem: \"Quais seriam as principais notícias da semana?\", \"Quais são os valores únicos da coluna Y?\",\n",
        "                \"Qual a correlação entre A e B?\". Evite utilizar esta ferramenta para solicitações mais amplas ou descritivas,\n",
        "                como informações gerais sobre o DataFrame, resumos estatísticos completos ou geração de gráficos — nesses casos,\n",
        "                use as ferramentas apropriadas.\"\"\"\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0AXWZDMqAD5u",
      "metadata": {
        "id": "0AXWZDMqAD5u"
      },
      "outputs": [],
      "source": [
        "def agente_langchain(llm:BaseChatModel, df:pd.DataFrame) -> dict:\n",
        "    ferramentas = [dataframe_python_code(df), get_current_time]\n",
        "\n",
        "    df_head:str = df.head().to_markdown()\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "                    input_variables=[\"input\", \"agent_scratchpad\", \"tools\", \"tool_names\"],\n",
        "                    partial_variables={\"df_head\": df_head},\n",
        "                    template = \"\"\"\n",
        "                        Você é um assistente que sempre responde em português.\n",
        "\n",
        "                        Você tem acesso a um dataframe pandas chamado `df`.\n",
        "                        Aqui estão as primeiras linhas do DataFrame, obtidas com `df.head().to_markdown()`:\n",
        "\n",
        "                        {df_head}\n",
        "\n",
        "                        Responda às seguintes perguntas da melhor forma possível.\n",
        "\n",
        "                        Para isso, você tem acesso às seguintes ferramentas:\n",
        "\n",
        "                        {tools}\n",
        "\n",
        "                        Use o seguinte formato:\n",
        "\n",
        "                        Question: a pergunta de entrada que você deve responder\n",
        "                        Thought: você deve sempre pensar no que fazer\n",
        "                        Action: a ação a ser tomada, deve ser uma das [{tool_names}]\n",
        "                        Action Input: a entrada para a ação\n",
        "                        Observation: o resultado da ação\n",
        "                        ... (este Thought/Action/Action Input/Observation pode se repetir N vezes)\n",
        "                        Thought: Agora eu sei a resposta final\n",
        "                        Final Answer: a resposta final para a pergunta de entrada original.\n",
        "\n",
        "                        Comece!\n",
        "\n",
        "                        Question: {input}\n",
        "                        Thought: {agent_scratchpad}\"\"\"\n",
        "                )\n",
        "\n",
        "    agente = create_react_agent(llm, ferramentas, prompt)\n",
        "    executor_do_agente = AgentExecutor(agent=agente, tools=ferramentas, handle_parsing_errors=True)\n",
        "    return executor_do_agente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kqYHnphgAKE3",
      "metadata": {
        "id": "kqYHnphgAKE3"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(f'{OUTPUT_DOCUMENTS_DIR}noticias_publicadas_ultimos_30d.csv')\n",
        "agente = agente_langchain(llm_openai, df)\n",
        "resposta = agente.invoke({\"input\": \"Quais foram as top 5 editorias com mais notícias na segunda semana de julho?\"})\n",
        "print(resposta.get(\"output\", \"Não encontrei a resposta\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sKgabg87ANVY",
      "metadata": {
        "id": "sKgabg87ANVY"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(f'{OUTPUT_DOCUMENTS_DIR}leitura_ultimos_5d_amostra.csv')\n",
        "agente = agente_langchain(llm_openai, df)\n",
        "resposta = agente.invoke({\"input\": \"Qual é o percentual de assinantes e não assinantes?\"})\n",
        "print(resposta.get(\"output\", \"Não encontrei a resposta\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PeGWFbjbAQU2",
      "metadata": {
        "id": "PeGWFbjbAQU2"
      },
      "outputs": [],
      "source": [
        "resposta = agente.invoke({\"input\": \"Gere um gráfico de pizza demonstrando cada tipo de usuário.\"})\n",
        "print(resposta.get(\"output\", \"Não encontrei a resposta\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "buSY0EEsq3op",
      "metadata": {
        "id": "buSY0EEsq3op"
      },
      "source": [
        "![grafico](https://middleware.datah.ai/agent_figura_10.png?12)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BFeRuvVBp7jn",
      "metadata": {
        "id": "BFeRuvVBp7jn"
      },
      "source": [
        "# Anexo I - Groq\n",
        "\n",
        "## Criando uma conta no Groq para conseguirmos uma **Free API Key** 😎\n",
        "\n",
        "![groq](data:image/svg+xml;base64,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)\\\n",
        "Fonte: https://groq.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5mYRI4xtqcqV",
      "metadata": {
        "id": "5mYRI4xtqcqV"
      },
      "source": [
        "**Groq** (https://groq.com) é uma empresa americana de inteligência artificial fundada em 2016 por ex-engenheiros do Google. Seu principal diferencial e inovação reside no desenvolvimento de um circuito integrado específico para aplicações de IA que eles chamam de **LPU** (**Language Processing Unit**), e hardware relacionado.\n",
        "\n",
        "A missão da Groq é acelerar o desempenho da inferência de cargas de trabalho de IA, ou seja, o processo de usar um modelo de IA já treinado para gerar previsões ou respostas. Eles se destacam por oferecer velocidade de processamento e eficiência incomparáveis, superando as GPUs (Graphics Processing Units) nesse aspecto, que foram originalmente projetadas para processamento gráfico e adaptadas para IA.\n",
        "\n",
        "**Pontos Chave sobre o Groq:**\n",
        "\n",
        "1. **LPU (Language Processing Unit)**: É o chip especializado da Groq. Diferente das GPUs, que são mais versáteis, as LPUs foram projetadas especificamente para a inferência de modelos de IA, especialmente Large Language Models (LLMs - Grandes Modelos de Linguagem). Essa especialização permite que as LPUs atinjam latências ultrabaixas e alto throughput (taxa de geração de tokens por segundo).\n",
        "2. **Velocidade Instantânea**: A Groq tem ganhado destaque no mercado por sua capacidade de gerar respostas de LLMs quase instantaneamente. Eles frequentemente demonstram que seus sistemas podem gerar centenas ou até milhares de tokens por segundo, um desempenho significativamente mais rápido do que muitas outras soluções disponíveis.\n",
        "3. **Eficiência Energética e Custo**: Além da velocidade, a arquitetura da LPU também é otimizada para maior eficiência energética e menor custo por inferência em comparação com as GPUs tradicionais.\n",
        "\n",
        "\\\n",
        "Em resumo, a **Groq** se posiciona como uma alternativa poderosa à **NVIDIA** no espaço de hardware de IA, focando especificamente em oferecer a inferência de LLMs mais rápida e eficiente do mercado por meio de sua inovadora tecnologia LPU."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1rsK7rPVqj4_",
      "metadata": {
        "id": "1rsK7rPVqj4_"
      },
      "source": [
        "Até a geração desse material o Groq oferece API Key gratuitas para desenvolvedores testarem diversos tipos de modelos. Para conseguir uma API, você deve se registrar no Groq e criar uma Free API Key.\n",
        "\n",
        "\\\n",
        "\n",
        "![groq](https://middleware.datah.ai/groq.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L_SBVyohqlMS",
      "metadata": {
        "id": "L_SBVyohqlMS"
      },
      "source": [
        "# Anexo II - Secrets\n",
        "\n",
        "Agora vamos configurar nossa API nos **Secrets** Google Colaboratory.\n",
        "\n",
        "\\\n",
        "![secrets](https://middleware.datah.ai/secrets.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D_-Uuiz1rZYI",
      "metadata": {
        "id": "D_-Uuiz1rZYI"
      },
      "source": [
        "# Anexo III - Repositório\n",
        "\n",
        "Baixe o projeto utilizando o link: https://knowledgebase.datah.com.br/cmpiovan/aula-agentes-2025.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QiqHm9K2rtu1",
      "metadata": {
        "id": "QiqHm9K2rtu1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "agent",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
